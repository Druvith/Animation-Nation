{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "15kydpSccjR9cAg_i7zCxkUOOQ0lUolAv",
      "authorship_tag": "ABX9TyNqJbJIIXobcKYWDGR6nq6w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a2c3f63cc72745e885043a30f928b719": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5046751dab82472dba302996c76c18fb",
              "IPY_MODEL_c544b4cb71394e919d43bec24ffe4861"
            ],
            "layout": "IPY_MODEL_c479444629db478ca3c3fddafb11e3f5"
          }
        },
        "5046751dab82472dba302996c76c18fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53fc57c52e56449e8e3ff3aba6b0be63",
            "placeholder": "​",
            "style": "IPY_MODEL_1d952be98c2c4af6accfe8dd26ff6d70",
            "value": "0.484 MB of 0.484 MB uploaded\r"
          }
        },
        "c544b4cb71394e919d43bec24ffe4861": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c38aaebcd4964ba8922d921ba6dac1de",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f3a33c3e387d4618b1e2839e26bd4723",
            "value": 1
          }
        },
        "c479444629db478ca3c3fddafb11e3f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53fc57c52e56449e8e3ff3aba6b0be63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d952be98c2c4af6accfe8dd26ff6d70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c38aaebcd4964ba8922d921ba6dac1de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3a33c3e387d4618b1e2839e26bd4723": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Druvith/Animation-Nation/blob/master/MakemoreMoE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcVWb1oLjqk7",
        "outputId": "7d6051af-7780-47c8-fc6e-98ae922eafbf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "HGm3PhYf4VG8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c346d99c-6cb7-4a1d-e66d-06131939341d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-21 15:10:40--  https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStories_all_data.tar.gz\n",
            "Resolving huggingface.co (huggingface.co)... 65.8.178.12, 65.8.178.93, 65.8.178.118, ...\n",
            "Connecting to huggingface.co (huggingface.co)|65.8.178.12|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/42/7f/427f7497b6c6596c18b46d5a72e61364fcad12aa433c60a0dbd4d344477b9d81/26cf7605aca15bc4ea6fa637256400d9d01317b28ed296172b2d1dd160cd7699?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27TinyStories_all_data.tar.gz%3B+filename%3D%22TinyStories_all_data.tar.gz%22%3B&response-content-type=application%2Fgzip&Expires=1711293040&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxMTI5MzA0MH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy80Mi83Zi80MjdmNzQ5N2I2YzY1OTZjMThiNDZkNWE3MmU2MTM2NGZjYWQxMmFhNDMzYzYwYTBkYmQ0ZDM0NDQ3N2I5ZDgxLzI2Y2Y3NjA1YWNhMTViYzRlYTZmYTYzNzI1NjQwMGQ5ZDAxMzE3YjI4ZWQyOTYxNzJiMmQxZGQxNjBjZDc2OTk%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=z-v5Mb72ntqO%7EgZtfSSU3c1TGQs6J9kNHZuLPDxmyHsQU3-kuE4%7Ey8%7EQjkSAcRtffYCdO5NzSDCRwgnliJ37Wrgq0MMFP8ayDV%7E2uukwUSGWtCfmUMz8Bly%7EYiaAjfdoTYtHMMSoQS8waS8Sp7D2yUuUMmIZCgBUWQwUm0ax2lBTJHPq0awB5unkiRC1nqYbPNfiJz-cM0vM-pGw2OreC4ugNOMiwl3NzRBRWLemYGThgbiNWPwVZkTOmrClPWdGDOUWoOVMu42N6y8bIsf8UgkzJ98GXthIwgIk7jG%7EQFo0kXaJ1TVOpmYeVEwPGgIbMdKp0Hyix7YTyjcfbkCZ%7Ew__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2024-03-21 15:10:40--  https://cdn-lfs.huggingface.co/repos/42/7f/427f7497b6c6596c18b46d5a72e61364fcad12aa433c60a0dbd4d344477b9d81/26cf7605aca15bc4ea6fa637256400d9d01317b28ed296172b2d1dd160cd7699?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27TinyStories_all_data.tar.gz%3B+filename%3D%22TinyStories_all_data.tar.gz%22%3B&response-content-type=application%2Fgzip&Expires=1711293040&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxMTI5MzA0MH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy80Mi83Zi80MjdmNzQ5N2I2YzY1OTZjMThiNDZkNWE3MmU2MTM2NGZjYWQxMmFhNDMzYzYwYTBkYmQ0ZDM0NDQ3N2I5ZDgxLzI2Y2Y3NjA1YWNhMTViYzRlYTZmYTYzNzI1NjQwMGQ5ZDAxMzE3YjI4ZWQyOTYxNzJiMmQxZGQxNjBjZDc2OTk%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=z-v5Mb72ntqO%7EgZtfSSU3c1TGQs6J9kNHZuLPDxmyHsQU3-kuE4%7Ey8%7EQjkSAcRtffYCdO5NzSDCRwgnliJ37Wrgq0MMFP8ayDV%7E2uukwUSGWtCfmUMz8Bly%7EYiaAjfdoTYtHMMSoQS8waS8Sp7D2yUuUMmIZCgBUWQwUm0ax2lBTJHPq0awB5unkiRC1nqYbPNfiJz-cM0vM-pGw2OreC4ugNOMiwl3NzRBRWLemYGThgbiNWPwVZkTOmrClPWdGDOUWoOVMu42N6y8bIsf8UgkzJ98GXthIwgIk7jG%7EQFo0kXaJ1TVOpmYeVEwPGgIbMdKp0Hyix7YTyjcfbkCZ%7Ew__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 108.157.173.105, 108.157.173.44, 108.157.173.84, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|108.157.173.105|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1608001638 (1.5G) [application/gzip]\n",
            "Saving to: ‘TinyStories_all_data.tar.gz.1’\n",
            "\n",
            "TinyStories_all_dat 100%[===================>]   1.50G  49.5MB/s    in 34s     \n",
            "\n",
            "2024-03-21 15:11:14 (45.2 MB/s) - ‘TinyStories_all_data.tar.gz.1’ saved [1608001638/1608001638]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Create the TinyStories directory if it doesn't exist\n",
        "os.makedirs(\"TinyStories\", exist_ok=True)\n",
        "\n",
        "# Download the archive\n",
        "!wget https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStories_all_data.tar.gz\n",
        "\n",
        "# Extract the archive into the TinyStories directory\n",
        "!tar -xzf TinyStories_all_data.tar.gz -C TinyStories"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import json"
      ],
      "metadata": {
        "id": "eT5aP5YUXYal"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6AU9OLhGhE7f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shard_filenames = sorted(glob.glob(os.path.join('TinyStories', \"*.json\")))\n",
        "with open(shard_filenames[0], \"r\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "stories = [x['story'] for x in data]\n",
        "text = \"\\n\".join(stories)"
      ],
      "metadata": {
        "id": "18W7yL67SZ59"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text)"
      ],
      "metadata": {
        "id": "8skg548ZQi-Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e87d2bce-61b6-4778-8f95-6779420611ed"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "77586884"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ve7QDheGiC99",
        "outputId": "bf6a874a-c6cc-4d91-881c-6f4d757cff7a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t\n",
            " !\"$%&'()*+,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]`abcdefghijklmnopqrstuvwxyz|~ éñ–—‘’“”…\n",
            "97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "stoi = { ch:i for i, ch in enumerate(chars) }\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: \"\".join([itos[x] for x in l])\n",
        "\n",
        "data = torch.tensor(encode(text), dtype = torch.long)\n"
      ],
      "metadata": {
        "id": "L71YqaT0pOkl"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "Ty9ih4OKiY_1"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 32\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "device= 'cuda' if torch.cuda.is_available() else 'cpu'\n"
      ],
      "metadata": {
        "id": "fJhXMdRBplSd"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1137)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ra8IjGacp3oZ",
        "outputId": "31dd9cfb-3e5f-4c9f-d456-c7d3b4ea6680"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7892d9d505b0>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MoeLayer(nn.Module):\n",
        "    def __init__(self, experts, gate, k=1):\n",
        "        super().__init__()\n",
        "        assert len(experts) > 0\n",
        "        self.experts = nn.ModuleList(experts)\n",
        "        self.gate = gate\n",
        "        self.k = k\n",
        "\n",
        "    def forward(self, inputs: torch.Tensor):\n",
        "        inputs_squashed = inputs.view(-1, inputs.shape[-1])\n",
        "        gate_logits = self.gate(inputs_squashed)\n",
        "        weights, selected_experts = torch.topk(\n",
        "            gate_logits, self.k\n",
        "        )\n",
        "        weights = nn.functional.softmax(\n",
        "            weights,\n",
        "            dim=1,\n",
        "            dtype=torch.float,\n",
        "        ).type_as(inputs)\n",
        "        results = torch.zeros_like(inputs_squashed)\n",
        "        for i, expert in enumerate(self.experts):\n",
        "            batch_idx, nth_expert = torch.where(selected_experts == i)\n",
        "            results[batch_idx] += weights[batch_idx, nth_expert, None] * expert(\n",
        "                inputs_squashed[batch_idx]\n",
        "            )\n",
        "        return results.view_as(inputs)"
      ],
      "metadata": {
        "id": "8YLSyzB6p5HY"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embed, head_size, bias = False)\n",
        "        self.query = nn.Linear(n_embed, head_size, bias = False)\n",
        "        self.value = nn.Linear(n_embed, head_size, bias = False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "        return out\n",
        "\n",
        "class MulitHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embed, n_embed)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x =  torch.cat([head(x) for head in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(x))\n",
        "        return out\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embed):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embed, 4* n_embed),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embed, n_embed),\n",
        "         nn.Dropout(dropout))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embed, n_head, num_experts=4):\n",
        "        super().__init__()\n",
        "        self.sa_head= MulitHeadAttention(n_head, n_embed//n_head)\n",
        "        self.ffw = MoeLayer(\n",
        "            experts=[FeedForward(n_embed) for _ in range(num_experts)],\n",
        "            gate=nn.Linear(n_embed, num_experts, bias=False),\n",
        "        )\n",
        "\n",
        "#         self.ffw=  FeedForward(n_embed)\n",
        "        self.ln1 = nn.LayerNorm(n_embed)\n",
        "        self.ln2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa_head(self.ln1(x))\n",
        "        x = x+self.ffw(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed, device=device)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embed, device=device)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embed, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        token_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T).to(device))\n",
        "        x = token_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        logits = self.lm_head(x)\n",
        "        if targets == None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokes):\n",
        "        for _ in range(max_new_tokes):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, loss = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim = -1)\n",
        "            idx_next = torch.multinomial(probs, num_samples = 1)\n",
        "            idx = torch.cat((idx, idx_next), dim = 1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "ekfwYoeup8oe"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jM1XrjmRruym",
        "outputId": "d40f6728-9456-4001-da12-8f912f4f0528"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.16.4-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.42-py3-none-any.whl (195 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.43.0-py2.py3-none-any.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.42 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.43.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "!wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOWvgQm4sLrS",
        "outputId": "3f6b7f97-bd22-4dd8-f7fa-f41f09e4f4af"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_embed = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.0\n",
        "# ------------"
      ],
      "metadata": {
        "id": "jCdMI0_wqFsq"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Transformer()\n",
        "optimizer = torch.optim.AdamW(model.parameters(),lr=learning_rate)\n",
        "print(sum(p.numel() for p in model.parameters()), 'total parameters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cyp02gpqIwh",
        "outputId": "e9bfaaa3-123e-4426-8595-fda390b8b325"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32090209 total parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSHMIBdmmMs9",
        "outputId": "74addda1-5b6c-4707-eb4b-2be54a11b6c7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (token_embedding_table): Embedding(97, 384)\n",
              "  (position_embedding_table): Embedding(256, 384)\n",
              "  (blocks): Sequential(\n",
              "    (0): Block(\n",
              "      (sa_head): MulitHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ffw): MoeLayer(\n",
              "        (experts): ModuleList(\n",
              "          (0-3): 4 x FeedForward(\n",
              "            (net): Sequential(\n",
              "              (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "              (1): ReLU()\n",
              "              (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "              (3): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (gate): Linear(in_features=384, out_features=4, bias=False)\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (1): Block(\n",
              "      (sa_head): MulitHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ffw): MoeLayer(\n",
              "        (experts): ModuleList(\n",
              "          (0-3): 4 x FeedForward(\n",
              "            (net): Sequential(\n",
              "              (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "              (1): ReLU()\n",
              "              (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "              (3): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (gate): Linear(in_features=384, out_features=4, bias=False)\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (2): Block(\n",
              "      (sa_head): MulitHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ffw): MoeLayer(\n",
              "        (experts): ModuleList(\n",
              "          (0-3): 4 x FeedForward(\n",
              "            (net): Sequential(\n",
              "              (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "              (1): ReLU()\n",
              "              (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "              (3): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (gate): Linear(in_features=384, out_features=4, bias=False)\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (3): Block(\n",
              "      (sa_head): MulitHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ffw): MoeLayer(\n",
              "        (experts): ModuleList(\n",
              "          (0-3): 4 x FeedForward(\n",
              "            (net): Sequential(\n",
              "              (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "              (1): ReLU()\n",
              "              (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "              (3): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (gate): Linear(in_features=384, out_features=4, bias=False)\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (4): Block(\n",
              "      (sa_head): MulitHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ffw): MoeLayer(\n",
              "        (experts): ModuleList(\n",
              "          (0-3): 4 x FeedForward(\n",
              "            (net): Sequential(\n",
              "              (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "              (1): ReLU()\n",
              "              (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "              (3): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (gate): Linear(in_features=384, out_features=4, bias=False)\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (5): Block(\n",
              "      (sa_head): MulitHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ffw): MoeLayer(\n",
              "        (experts): ModuleList(\n",
              "          (0-3): 4 x FeedForward(\n",
              "            (net): Sequential(\n",
              "              (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "              (1): ReLU()\n",
              "              (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "              (3): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (gate): Linear(in_features=384, out_features=4, bias=False)\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=384, out_features=97, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            X = X.to(device)\n",
        "            Y = Y.to(device)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "6mCwnSVcqLMr"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)\n",
        "\n",
        "wandb.init(\n",
        "    # set the wandb project where this run will be logged\n",
        "    project=\"mixture of experts\",\n",
        "\n",
        "    # track hyperparameters and run metadata\n",
        "    config={\n",
        "    'batch_size': batch_size,\n",
        "    'block_size': block_size,\n",
        "    'max_iters': max_iters,\n",
        "    'eval_interval': eval_interval,\n",
        "    'learning_rate': learning_rate,\n",
        "    'device': device,\n",
        "    'eval_iters': eval_iters,\n",
        "    'n_embd': n_embd,\n",
        "    'n_embed': n_embed,\n",
        "    'n_head': n_head,\n",
        "    'n_layer': n_layer,\n",
        "    'dropout': dropout\n",
        "    }\n",
        ")\n",
        "\n",
        "wandb.watch(model)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % 100 == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        wandb.log({'train_loss': losses['train'], 'val_loss': losses['val']}, step=iter)\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "    xb = xb.to(device)\n",
        "    yb = yb.to(device)\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "model_dir = 'Mixture of experts models'\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "final_model_path = os.path.join(model_dir, 'moe_model.pth')\n",
        "torch.save(model.state_dict(), final_model_path)\n",
        "print('Final trained model saved!')\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a2c3f63cc72745e885043a30f928b719",
            "5046751dab82472dba302996c76c18fb",
            "c544b4cb71394e919d43bec24ffe4861",
            "c479444629db478ca3c3fddafb11e3f5",
            "53fc57c52e56449e8e3ff3aba6b0be63",
            "1d952be98c2c4af6accfe8dd26ff6d70",
            "c38aaebcd4964ba8922d921ba6dac1de",
            "f3a33c3e387d4618b1e2839e26bd4723"
          ]
        },
        "id": "F4xAAPz1qM4F",
        "outputId": "da7a2bad-bdcf-420d-a14e-7db8d218d574"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.4"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240321_063314-q242zy49</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/druvithlgowda00/Mixture%20of%20experts/runs/q242zy49' target=\"_blank\">peach-sea-1</a></strong> to <a href='https://wandb.ai/druvithlgowda00/Mixture%20of%20experts' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/druvithlgowda00/Mixture%20of%20experts' target=\"_blank\">https://wandb.ai/druvithlgowda00/Mixture%20of%20experts</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/druvithlgowda00/Mixture%20of%20experts/runs/q242zy49' target=\"_blank\">https://wandb.ai/druvithlgowda00/Mixture%20of%20experts/runs/q242zy49</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.8669, val loss 4.8668\n",
            "step 100: train loss 2.3447, val loss 2.3453\n",
            "step 200: train loss 2.3033, val loss 2.3023\n",
            "step 300: train loss 2.2774, val loss 2.2778\n",
            "step 400: train loss 2.2385, val loss 2.2381\n",
            "step 500: train loss 2.1665, val loss 2.1676\n",
            "step 600: train loss 2.0180, val loss 2.0165\n",
            "step 700: train loss 1.8367, val loss 1.8385\n",
            "step 800: train loss 1.7020, val loss 1.7083\n",
            "step 900: train loss 1.6097, val loss 1.6092\n",
            "step 1000: train loss 1.5354, val loss 1.5375\n",
            "step 1100: train loss 1.4760, val loss 1.4777\n",
            "step 1200: train loss 1.4261, val loss 1.4225\n",
            "step 1300: train loss 1.3794, val loss 1.3810\n",
            "step 1400: train loss 1.3411, val loss 1.3403\n",
            "step 1500: train loss 1.3069, val loss 1.3092\n",
            "step 1600: train loss 1.2763, val loss 1.2792\n",
            "step 1700: train loss 1.2519, val loss 1.2577\n",
            "step 1800: train loss 1.2294, val loss 1.2297\n",
            "step 1900: train loss 1.2060, val loss 1.2077\n",
            "step 2000: train loss 1.1881, val loss 1.1883\n",
            "step 2100: train loss 1.1667, val loss 1.1700\n",
            "step 2200: train loss 1.1523, val loss 1.1545\n",
            "step 2300: train loss 1.1361, val loss 1.1427\n",
            "step 2400: train loss 1.1194, val loss 1.1244\n",
            "step 2500: train loss 1.1087, val loss 1.1112\n",
            "step 2600: train loss 1.0967, val loss 1.0972\n",
            "step 2700: train loss 1.0830, val loss 1.0810\n",
            "step 2800: train loss 1.0691, val loss 1.0732\n",
            "step 2900: train loss 1.0610, val loss 1.0591\n",
            "step 3000: train loss 1.0552, val loss 1.0535\n",
            "step 3100: train loss 1.0414, val loss 1.0463\n",
            "step 3200: train loss 1.0307, val loss 1.0322\n",
            "step 3300: train loss 1.0232, val loss 1.0248\n",
            "step 3400: train loss 1.0139, val loss 1.0144\n",
            "step 3500: train loss 1.0040, val loss 1.0081\n",
            "step 3600: train loss 0.9986, val loss 1.0010\n",
            "step 3700: train loss 0.9914, val loss 0.9935\n",
            "step 3800: train loss 0.9815, val loss 0.9873\n",
            "step 3900: train loss 0.9787, val loss 0.9815\n",
            "step 4000: train loss 0.9709, val loss 0.9720\n",
            "step 4100: train loss 0.9650, val loss 0.9679\n",
            "step 4200: train loss 0.9550, val loss 0.9591\n",
            "step 4300: train loss 0.9524, val loss 0.9540\n",
            "step 4400: train loss 0.9476, val loss 0.9505\n",
            "step 4500: train loss 0.9407, val loss 0.9453\n",
            "step 4600: train loss 0.9348, val loss 0.9401\n",
            "step 4700: train loss 0.9280, val loss 0.9303\n",
            "step 4800: train loss 0.9266, val loss 0.9262\n",
            "step 4900: train loss 0.9192, val loss 0.9227\n",
            "step 4999: train loss 0.9171, val loss 0.9193\n",
            "Final trained model saved!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a2c3f63cc72745e885043a30f928b719"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Control-C detected -- Run data was not synced\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sum(p.numel() for p in model.parameters()), 'total parameters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sdMcU4YrLD3",
        "outputId": "ef01f771-6be0-4636-da08-8bd7daec9365"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32090209 total parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generation\n",
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "print(decode(model.generate(context, max_new_tokes=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9N71t7VHhvQ",
        "outputId": "6fb23f23-0baf-469b-d895-1a45ac75136d"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tt's hanga nepagerous.\"\n",
            "Lily says bling and asked, \"I can you something wre next time!\" Max says. They want to watch the snows find the other rock. He made a surprise or scared the water. The next day, Max's friend, Benny. In the tack and crew never long that chighs heals knew things, never lemss ok fun on the game. Timmy stopped creames and counted any happy.\n",
            "Once upon a time, there was a big fanama love in his was so not a went bace. He lived its forest. He had a little many closer and watch th\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_dir = 'Mixture of experts models'\n",
        "final_model_path = os.path.join(model_dir, 'moe_model1.pth')\n",
        "torch.save(model.state_dict(), final_model_path)\n"
      ],
      "metadata": {
        "id": "BuODNiXnJNJZ"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J6DQHwWwJXhf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}